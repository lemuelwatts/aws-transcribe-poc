# Copyright 2025 Booz Allen Hamilton.
#
# Booz Allen Hamilton Confidential Information.
#
# The contents of this file are the intellectual property of
# Booz Allen Hamilton, Inc. ("BAH") and are subject to copyright protection
# under the laws of the United States and other countries.
#
# You acknowledge that misappropriation, misuse, or redistribution of content
# on the file could cause irreparable harm to BAH and/or to third parties.
#
# You may not copy, reproduce, distribute, publish, display, execute, modify,
# create derivative works of, transmit, sell or offer for resale, or in any way
# exploit any part of this code or program without BAH's express written permission.
#
# The contents of this code or program contains code
# that is itself or was created using artificial intelligence.
#
# To the best of our knowledge, this code does not infringe third-party intellectual
# property rights, contain errors, inaccuracies, bias, or security concerns.
#
# However, Booz Allen does not warrant, claim, or provide any implied
# or express warranty for the aforementioned, nor of merchantability
# or fitness for purpose.
#
# Booz Allen expressly limits liability, whether by contract, tort or in equity
# for any damage or harm caused by use of this artificial intelligence code or program.
#
# Booz Allen is providing this code or program "as is" with the understanding
# that any separately negotiated standards of performance for said code
# or program will be met for the duration of any applicable contract under which
# the code or program is provided.

"""This file will hold logic for analyzer LLM"""

import datetime
import json
import logging
import os
import re
from pathlib import Path

import boto3
from botocore.exceptions import ClientError
from pydantic import BaseModel, Field

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Inconsistency(BaseModel):
    """A detected inconsistency between transcript and notes or between speakers."""

    description: str = Field(description="What the inconsistency is")
    evidence: str = Field(description="Quotes or references from the meeting data")
    severity: str = Field(description="low, medium, or high")


class FoundInconsistencies(BaseModel):
    """List of inconsistencies found in the meeting."""

    inconsistencies: list[Inconsistency] = Field(default_factory=list)


class ComplianceIssue(BaseModel):
    """A potential compliance or process concern."""

    issue: str = Field(description="Description of the compliance concern")
    context: str = Field(description="What was said or noted that triggered this")
    recommendation: str = Field(description="Suggested remediation")


class ComplianceReport(BaseModel):
    """Compliance check results."""

    issues: list[ComplianceIssue] = Field(default_factory=list)
    compliant: bool = Field(description="True if no issues found")


class ImprovementOpportunity(BaseModel):
    """A suggestion for improving meeting effectiveness."""

    area: str = Field(
        description="Category: time management, preparation, follow-up, etc."
    )
    observation: str = Field(description="What was observed")
    suggestion: str = Field(description="How to improve")


class MeetingImprovements(BaseModel):
    """Meeting improvement analysis."""

    opportunities: list[ImprovementOpportunity] = Field(default_factory=list)


class GeneralizedSummary(BaseModel):
    """Structure for Summary Generated by model"""

    overview: str = Field(description="Brief BLUF-style overview of the meeting")
    proposed_title: str = Field(description="AI-generated title for the conversation")
    summary: str = Field(description="Long-form detailed summary")


class ActionItem(BaseModel):
    """Composition of an action item"""

    task: str = Field(description="Task that needs to be completed")
    task_owner: str = Field(
        description="The owner that needs to complete the associated task"
    )


class GeneratedActionItems(BaseModel):
    """Structure for list of action items"""

    action_items: list[ActionItem]


class FinalReport(BaseModel):
    """Response for the final report generated by the model."""

    summary: GeneralizedSummary
    action_items: GeneratedActionItems
    meeting_improvements: MeetingImprovements | None = None
    found_inconsistencies: FoundInconsistencies | None = None
    compliance_issues: ComplianceReport | None = None


class AnalyzerService:
    """Service for analyzing meeting transcripts using AWS Bedrock."""

    def __init__(self, input_file: str) -> None:
        """Initialize the analyzer service."""
        self.region = os.environ.get("AWS_REGION", "us-east-1")
        self.bedrock_client = boto3.client("bedrock-runtime", region_name=self.region)
        self.model_id = os.environ.get("BEDROCK_MODEL_ID", "amazon.nova-pro-v1:0")
        self.input_file = input_file
        self.config = self._read_config()
        self.compliance_rules = self.config.get("compliance_rules", [])

    def _read_config(self):
        config_path = Path(__file__).parent / "analyzer_config.json"
        with open(config_path) as f:
            return json.load(f)

    def _read_notes(self):
        with open(self.input_file) as f:
            meeting_data = json.load(f)
        return meeting_data

    def _extract_json(self, text: str) -> str:
        """Extract JSON from response, stripping markdown code fences if present."""
        # Remove markdown code like```json ... ``` or ``` ... ```
        fence_pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
        match = re.search(fence_pattern, text, flags=re.IGNORECASE)
        if match:
            # Extract content inside the fence
            content = match.group(1).strip()
            return content
        else:
            # Try extracting (correctly) a JSON object present anywhere
            brace_match = re.search(r"(\{[\s\S]*\})", text)
            if brace_match:
                return brace_match.group(1).strip()
            # As fallback, just return the whole text (may fail when parsing JSON)
            return text.strip()

    def build_prompt(
        self,
        task_instruction: str,
        output_schema: type[BaseModel],
        example_override: str = None,
    ):
        """Modular function that takes in an anlytical option prompt and returns results from model invocation."""
        logger.debug("inside build prompt")

        if example_override:  # build nested examples for list of ActionItems
            schema_json = example_override
        else:  # otherwise dynamically build the schema from the pydantic model passed in
            schema_information = {}
            for name, field_info in output_schema.model_fields.items():
                schema_information[name] = f"<{field_info.description}>"

            schema_json = json.dumps(schema_information, indent=2)

        prompt_template = """
        Analyze the following meeting data according to the specific guidance provided.

        Meeting data: {meeting_content}

        Your job: {task_instruction}

        Think step by step then respond ONLY with a JSON object in this exact format (fill in the values):
        {schema_json}

        Output ONLY the JSON, no other text.
        """
        meeting_data = self._read_notes()

        prompt = prompt_template.format(
            meeting_content=json.dumps(meeting_data),
            task_instruction=task_instruction,
            schema_json=schema_json,
        )
        logger.debug(f"leaving build_prompt - {prompt}")
        return prompt

    def call_model(
        self,
        task_instruciton: str,
        output_schema: type[BaseModel],
        example_override: str = None,
    ):
        """Invokes model with request built from task instruction and matching schema."""
        logger.debug("inside call_model")

        prompt = self.build_prompt(task_instruciton, output_schema, example_override)

        logger.debug("building request")

        request_body = {
            "messages": [
                {
                    "role": "user",
                    "content": [{"text": prompt}],
                }
            ],
            "inferenceConfig": {
                "maxTokens": 4096,
                "temperature": 0,
                "topP": 0.9,
            },
        }

        request = json.dumps(request_body)

        logger.debug("invoking model")
        try:
            response = self.bedrock_client.invoke_model(
                modelId=self.model_id, body=request
            )
        except ClientError as e:
            raise RuntimeError(f"Can't invoke '{self.model_id}': {e}") from e

        model_response = json.loads(response["body"].read())

        response_text = model_response["output"]["message"]["content"][0]["text"]

        # clean model response - strip markdown code fences if present
        json_text = self._extract_json(response_text)

        report = output_schema.model_validate_json(json_text)
        logger.debug("leaving call_model")
        return report

    def run_analysis(self, save_report: bool = False):
        """Build final report by putting pieces together"""
        logger.debug("inside run_analysis")
        summary = self.generate_summary()
        logger.debug(f"Summary is: {summary}")
        action_items = self.generate_action_items()
        logger.debug(f"action items: {action_items}")

        logger.debug(
            f"Summary type: {type(summary)} | Action Items: {type(action_items)}"
        )

        # optional analytics based on config
        inconsistencies = None
        compliance = None
        improvements = None

        if self.config.get("find_inconsistencies"):
            inconsistencies = self.find_inconsistencies()
        if self.config.get("compliance_check"):
            compliance = self.compliance_check()
        if self.config.get("improvement_opportunities"):
            improvements = self.find_improvement_opportunities()

        # put pieces together to build FinalReport
        final_report = FinalReport(
            summary=summary,
            action_items=action_items,
            found_inconsistencies=inconsistencies,
            compliance_issues=compliance,
            meeting_improvements=improvements,
        )

        logger.info(final_report)
        output_path = None
        if save_report:
            output_path = self.save_report(final_report)

        return final_report, output_path

    def save_report(self, report):
        """Write out report to path; later can be to s3 bucket"""
        # get time to add to filename
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        # get title from GeneralizedSummary
        title = report.summary.proposed_title
        cleaned_title = "".join(
            c if c.isalnum() or c in ("-", "_") else "_" for c in title
        )
        cleaned_title = cleaned_title[:50]  # truncate if too long

        Path("output").mkdir(
            parents=True,
        )

        output_file_path = f"output/{cleaned_title}_{timestamp}.json"

        try:
            with open(output_file_path, "w") as f:
                f.write(report.model_dump_json(indent=2))
        except Exception as e:
            error = str(e)
            logger.error(
                f"Error trying to write report to {output_file_path} : {error}"
            )
            raise

        return output_file_path

    ## define methods - analysis options, defined as components ##

    ### defaults, analysis always run
    def generate_summary(self) -> GeneralizedSummary:
        """Generates a summary given transcript and meeting notes
        Two step prompting:
            1. Send model context and task
            2. Receive initial results and have a second call to model to verify results
        """
        task = """
        Your main task is to generate a summary of the meeting provided.

        1. Review the entire transcript and think about the theme and purpose of the meeting.
        2. Review the notes from the attendees.
        3. Summarize both notes and the transcript into one cohesive, thorough summary.
        4. Extract a BLUF and title that matches your summary.
        """
        logger.debug("inside generate_summary; task defined, calling call_model")
        draft = self.call_model(task, GeneralizedSummary)
        logger.debug(f"draft received, calling verify task;\ndraft: {draft}\n")

        verify_task = f"""You are tasked with fact-checking this summary against the original
        meeting data.
        Summary to verify: {draft.model_dump_json()}

        Check:
        - is every statement in the summary backed by evidence in the meeting notes?
        - is anything missing?

        If the summary is accurate, return the summary unchanged.
        If issues exist, return a corrected version.
        """

        verified_response = self.call_model(verify_task, GeneralizedSummary)

        return verified_response

    def generate_action_items(self) -> GeneratedActionItems:
        """Generates a list of action items from the meeting.
        Two step prompting:
            1. Send model context and task with an example
            2. Receive initial results and have a second call to model to verify results
        """
        task = """
            Your main task is to extract action items from the meeting.

            1. Review the transcript for any commitments, assignments, or follow-ups.
            2. Review attendee notes for tasks they captured.
            3. For each action item, identify WHO is responsible and WHAT they need to do.
            4. Only include concrete, actionable tasks with clear owners.
        """

        example = json.dumps(
            {
                "action_items": [
                    {
                        "task": "<specific task to complete>",
                        "task_owner": "<person resonsible>",
                    },
                    {
                        "task": "<another specific task to complete>",
                        "task_owner": "<another person resonsible>",
                    },
                ]
            }
        )
        draft = self.call_model(task, GeneratedActionItems, example_override=example)

        verify_task = f"""You are tasked with verifying these action items against the original meeting data.
        Action items to verify: {draft.model_dump_json()}

        Check:
        - Is each task actually mentioned or implied in the meeting?
        - Is the owner correctly identified?
        - Are any action items missing?

        If accurate, return unchanged.
        If issues exist, return a corrected version.
        """

        verified = self.call_model(
            verify_task, GeneratedActionItems, example_override=example
        )
        return verified

    ### configurable analytics ###

    # changed from "find inconsistencies" which made model very desperate for finding them..
    #   to "verify consistency"; note this still flags debates
    def find_inconsistencies(self) -> FoundInconsistencies:
        """Find inconsistencies between transcript and notes or between speakers.
        Two step prompting:
            1. Send model context and task with an example
            2. Receive initial results and have a second call to model to verify results
        """
        task = """
        Review the meeting transcript and attendee notes.

        Your job: VERIFY consistency of RECORDED FACTS. You have teammates that are reviewing for compliance and improvements.

        You are looking for contradictions in what was documented—NOT debates about what SHOULD be done.

        ONLY flag if:
        - Someone claims a past decision/date/number, but someone else claims a DIFFERENT past decision/date/number
        - A person's notes contradict what they said in the transcript
        - Two people state incompatible facts about the SAME completed event

        DO NOT flag:
        - Proposals being debated (e.g., "skip review" vs "we need review")
        - Disagreements about priorities or urgency
        - Policy questions or compliance concerns (these belong in compliance_check)

        For each statement in the transcript or notes, ask:
        "Is this contradicted elsewhere in the meeting data?"

        Only flag an item if you find a direct contradiction that cannot both be true.

        Think step by step:
        1. List all factual claims (dates, numbers, decisions, assignments)
        2. Disregard subjective statements, suggestions, proposals, debates.
        3. Check each claim against all other data
        4. Flag ONLY if you find a claim that directly contradicts another
        5. Do NOT flag paraphrases, abbreviations, or different perspectives on the same issue

        EXAMPLE OF INCONSISTENCY:
        - Person A: "We agreed on 2 weeks" vs Person B: "We agreed on 3 weeks"
        (Both claim a past decision, but it can't be both)

        NOT AN INCONSISTENCY:
        - Person A: "We should skip the review" vs Person B: "We shouldn't skip it"
        (This is a debate happening now; both people said these things)

        Return empty list if meeting is internally consistent.
        """

        example = json.dumps(
            {
                "inconsistencies": [
                    {
                        "description": "<what facts conflict>",
                        "evidence": "<Quote A vs Quote B - cannot both be true>",
                        "severity": "<low|medium|high>",
                    }
                ]
            },
            indent=2,
        )

        return self.call_model(task, FoundInconsistencies, example_override=example)

    # MULTI-STEP orchestrating with the prompt
    ## step 1: high-level look for potential inconsistencies
    ## step 2: if potential inconsistencies -> explore to determine if fr; else return empty list
    ## step 3: verification; is this legit?
    def compliance_check(self) -> ComplianceReport:
        """Check for compliance or process concerns in the meeting.
        Three step prompting:
            1. Send model context and task with an example, looking for a binary response: yes/no
                whether model found a potential violation
            2. Update task and context to potential violation checking in depth if potential violations
                found in first model call. If none, return empty list of compliance report and True for compliant
            2. Receive results and have a second call to model to verify results
        """
        # does meeting contain violations?
        gate_prompt = """
        Review this meeting data. Answer YES or NO:

        Does anyone explicitly propose to:
        - Skip a required approval, review, or sign-off?
        - Bypass a stated policy or process?
        - Handle sensitive data without required controls?

        Look for phrases like "skip the review", "bypass legal", "proceed without approval".

        Normal blockers and dependencies are NOT violations.

        Answer: YES or NO only.
        """

        meeting_data = self._read_notes()
        gate_body = {
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"text": gate_prompt + "\n\n" + json.dumps(meeting_data)}
                    ],
                }
            ],
            "inferenceConfig": {"maxTokens": 10, "temperature": 0},
        }

        logger.info("Running compliance gate check...")
        gate_response = self.bedrock_client.invoke_model(
            modelId=self.model_id, body=json.dumps(gate_body)
        )
        gate_text = json.loads(gate_response["body"].read())["output"]["message"][
            "content"
        ][0]["text"].strip()
        logger.info(f"Gate response: {gate_text}")

        # If NO violations detected, return compliant
        if "NO" in gate_text.upper():
            logger.info("No violations detected at gate - returning compliant")
            return ComplianceReport(issues=[], compliant=True)

        logger.info("Potential violations detected - running detailed extraction")

        # extract violations if potential violations detected
        if self.compliance_rules:
            rules_text = "\n".join(f"- {rule}" for rule in self.compliance_rules)
            rules_section = f"CHECK AGAINST THESE RULES:\n{rules_text}"
        else:
            rules_section = "Use general compliance standards (legal, security, procurement approvals)."

        task = f"""
        Review the meeting for COMPLIANCE violations only.

        COMPLIANCE means: breaking organizational rules, policies, or legal requirements.
        Examples: skipping required approvals, mishandling sensitive data, bypassing procurement.

        {rules_section}

        FLAG ONLY:
        - Someone explicitly proposes to skip a required approval/review
        - A decision is made that violates a stated policy
        - Sensitive data (PII, PHI) handled without required controls

        DO NOT FLAG (these are NOT compliance issues):
        - Blockers or dependencies (normal work)
        - Technical debt (test coverage, code quality)
        - Waiting for feedback (that's following process)
        - Action items or tasks

        FLAG ONLY if you can provide:
        1. The specific rule being violated
        2. A direct quote where someone proposes violating it

        "Credentials" or "staging access" shared between team members is NORMAL — not a compliance issue.

        If no policy violations exist, return: {{"issues": [], "compliant": true}}
        """

        example = json.dumps(
            {
                "issues": [
                    {
                        "issue": "<description of compliance concern>",
                        "context": "<what was said that triggered this>",
                        "recommendation": "<suggested remediation>",
                    }
                ],
                "compliant": False,
            },
            indent=2,
        )

        draft = self.call_model(task, ComplianceReport, example_override=example)

        verify_task = f"""Verify these compliance findings strictly:
        {draft.model_dump_json()}

        Remove any issues where:
        1. Someone proposed it but the meeting ended with NO DECISION made
        2. OR someone objected and a safer path was chosen

        Keep only issues where someone EXPLICITLY DECIDED to violate policy.

        Set compliant=true if no issues remain.
        """

        return self.call_model(verify_task, ComplianceReport, example_override=example)

    def find_improvement_opportunities(self) -> MeetingImprovements:
        """Analyze meeting effectiveness and suggest improvements.
        Two step prompting:
            1. Send model context and task with an example
            2. Receive initial results and have a second call to model to verify results

        """
        task = """
        Analyze meeting effectiveness and identify improvement opportunities.

        LOOK FOR:
        - Time management: tangents, repeated discussions, topics without resolution
        - Preparation gaps: missing information that blocked decisions
        - Follow-up clarity: vague assignments, missing deadlines, unclear owners
        - Participation: one person dominating, others silent
        - Decision-making: deferred decisions, lack of consensus

        Be constructive. Only flag actionable improvements.
        Return empty list if meeting was well-run.
        """

        example = json.dumps(
            {
                "opportunities": [
                    {
                        "area": "<category: time management, preparation, follow-up, etc.>",
                        "observation": "<what was observed>",
                        "suggestion": "<specific improvement>",
                    }
                ]
            },
            indent=2,
        )

        draft = self.call_model(task, MeetingImprovements, example_override=example)

        verify_task = f"""Review these improvement suggestions:
        {draft.model_dump_json()}

        For each:
        - Is the observation accurate based on the meeting data?
        - Is the suggestion actionable and constructive?

        Remove nitpicks or unsupported observations.
        Keep genuinely helpful improvements.
        """

        return self.call_model(
            verify_task, MeetingImprovements, example_override=example
        )
